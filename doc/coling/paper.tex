\documentclass[10pt,a5paper,twoside]{article}
\usepackage{coling2012}
\title{Article Title: document template for COLING-2012}
%\author{$Malin~AHLBERG~~~Gerlof~BOUMA$\\
\author{$Anonymous$\\
{\small  
  %Språkbanken \\
  %Department of Swedish \\
  %University of Gothenburg \\
  INSTITUTE\_1, address 1\\
  INSTITUTE\_1, address 1\\
  \texttt{email@domain} \\ 
  %\texttt{malin.ahlberg@svenska.gu.se, gerlof.bouma@svenska.gu.se} \\ 
}}

\begin{document}
\maketitle
%% The first mandatory ABSTRACT (\abstractEn) section below is for the English language
\abstractEn{  %ABSTRACT}{
Place for the abstract in English (maximum 150 words). \\	
}

%for authors: the line below is for instruction purposes and can be commented
%\textbf{AT SUBMISSION TIME, PLEASE ANONYMISE everywhere, REPLACING NAMES BY DUMMIES LIKE John DOE\_1, Jack DOE\_2.}

%for authors: the abstract section below is optional and can be commented otherwise


%for authors: if only English language option is chosen comment the \abstractOL section above and use \keywordsEn below 

\keywordsEn{Here a list of keywords in English}


\newpage
%================================================================
%\section{Optional condensed 2-page version in $L_{2}$ (1st section if present)}

\section{Introduction}
text text\\
referenser i massor\\
\section{Background}
om problemet, blabla

\subsection{Algorithm-section}
\subsubsection{Regler, anagram, levenshtein}
regel från lexikon \\
ref till andra pappret\\
Levenshtein-variant\\
upp till tri-gram, n-m-gram, may include word boundary.
anagram\\
Copy-paste, utveckla lite:\\
Calculating the edit distance between a source word and each of the
about 55,000 target dictionary entries is too expensive
to be applied to million word corpora. We therefore
use an \textit{anagram hashing} filter \cite{Reynaert2010} to
cut down the number of exact edit distance calculations.  The filter
computes character based hashes for the source words and for the target
entries. Character edits such as substitution can be performed
numerically on these hashes. After applying a numerical edit on a
source hash, we have a new hash value which we can compare against the
hash table representing the dictionary or apply further edits on. The
hash function used loses information about the order of characters in
a string, hence the name \textit{anagram hash}, which means the filter
is optimistic as to what the actual number of required edits between
source and target is.
Pseudoalgoritm om hur det går till?

\subsubsection{Graph}
\textit{Why?}
A large set of rules, a very large set of combinations. Need to restrict, filter
out the best. Previously generated a number of anagrams, based on the number of
operations carried out. Now implemented best-first search, as two operations may
be cheaper than one single expensive, but may be filtered out.

\textit{Overlook}
The problem is represented as a directed acyclic graph $G = (V,E)$, where each
node $n \in V$ represents a hash value $n_h$, that is, a (possibly empty) set of variants
of the source word $w$.
The source node represents the hash value of the original word, where no
operations have been carried out.
An edge $nm \in E$ describes the edit operation manipulating $n_h$ to $m_h$
and is weighted by the cost of that operation.
The set of edit operations needed to transform $w$ into $n_h$ is consequently %($n_{operations}$) 
described by the path from the source node to $n$ and we construct the
graph so that there is exactly one such path.

The described graph is also a rooted tree with the start node as root.
The nodes at depth $i$ represents a set of hash values achieved by applying $i$
operations performed on $w$.
%a path leading to a specific node corresponds to a serie of operations manipulating the source word. 
As we will prohibit paths %with an operation modifying letters already used
applying more than one operation to the same letter, the order in which we
carry out the operations will not effect the result.
%, and we can let each node have exactly one edge leading to it. 
%for convenience, we index the nodes by the edge pointing at it.
% fel: each node is indexed by the $a$ is connected to $b$, $ab \in E$, iff $operations(a) \subset operations(b)$ 
%$ab \in E$, implies $cost(op(b) < cost(op(a))$). (eller hur vill man säga det?)
%The cost of the edge $ab$ is the cost of $op(b)-op(a)$.  
%In other words, an edge correpsonds to carrying out one more edit operation
%...(nice explanation).

Starting from the list of all rules applicable to $w$, we construct a neighbor
list describing the graph. A rule expresses an available operation for $w$, its
cost as well as the letters that are effected. 
%At the first stage, we calculate a list of rules, or operations, $R$, 
%expressing available operations for the word $w$ and their cost. This will
%later serve as a neighbour list. 
The list is sorted by costs. %and each rule corresponds to an edge.
For each node $n$ we inspect the operation $o$ described by the edge $vn \in E$
(remember that there is exactly one such edge) and add one edge from $n$ for
each rule in the list starting from $o$+1.
%children be all nodes reached by following the edges in $R[j]_{i<j}$.
%the edge leading to it %(its most expensive operation) 
%$i(n) = mn \in E$, then $nm \in E$ if $i(n)<i(m)$, 
%the edges and each node points at its own index in the list (index = its most expensive operation).

The problem is now reduced?? to identifying the cheapest path in the graph. 
There is no target node, instead we stop at each step and
inspecting the present node and the variation represented by the path leading
to it. This is done by applying a variant of Dijkstra's algorithm. 


\textit{Nicefications}
The graph described above is usually very big, (example of number of nodes/edges in a
graph), and it is not feasible to even calculate it all at once, especially as we
normally only traverse it at a comparatively shallow depth before having found $k$
number of variants.
%We restrict the evaluated part of the graph, 
Starting from the source node only and the rule list we iteratively evaluate
the graph as we step through it. For each visited node, we do not (as normally
in Dijkstra) push all its children to the priority queue. We know
that the only two new (värda) options after having marked this node as visited, is either to
continue to its cheapest child, or to instead visit the cheapest child of its parent, ie.
this node's sibling. Hence, at each step, we only push the
cheapest child and cheapest sibling of the present node to the priority queue.
Also, a node is only added to the priority queue if the weight of the path
leading to it is below 2 (the highest edit distance allowed).

\textit{Deltaize etc}
In order to keep time and memory consumption down, we compress the graph by
merging rules which result in the same anagram hash. This keeps graph size down
mycket! Number? 
%Our most expensive operation is to expand the graph by adding more nodes to the
%priority queue...
%(, and does not effect the resulting,)
This is done while constructing the neighbor list.
Whenever two rules result in the same hash, we only add this as one rule, with 
the weight set to the minimum of the two. This makes our algorithm optimistic, 
the first time an anagram is returned, it will be ranked with a lower edit distance
than what the Levenshtein will ge den.

%Slår vi ihop regler som har samma hash, olika värde? Ja -> billigast
%                            samma hash, olika bokstäver? Ja -> lägg till i bitmap
%                                    ta bara med dem som har minst inverkan 

As mentioned, we must % restrict the edges from $n$ in order to
make sure not to choose paths where the operations induced by operate more than
once on every letter. We will hence need to filter the neighbor list before
applying it to a node.
%When traversing the graph, 
%Further, we wish to make sure that the set of operations do not overlap, so
%that a character cannot be manipulated twice. 
On the other hand, we must make sure
that a rule which can be applied twice on a word is not only represented once.
This information is encoded as bitmasks, where a bit is put to 1 if the
corresponding character is used. Each rule keeps information of which letters
it uses, and each node keeps information about which letters that have already been
used. When merging the rules, as above, we get lists of bitmasks, showing the different
possibilities of how to apply the rule, but when ever one bitmask is 'included' in another,
we only keep the bitmask that changes the least number of letters. This is implemented as
'or' ($\|$): \\ 
\begin{small}\verb- if bitmask1 || bitmask2 == bitmask1; then discard bitmask2 - 
\end{small} \\
%(gör vi fortfarande så?)
Consequently, a node may also have more than one possible bitmaps.
To check whether a rule and a node is compatible, we use 'not and' ($\lnot \&$):
\begin{small}\begin{verbatim}
for (a,b) in product(bitmap_{node},bitmask_{rule}):
  if $\lnot$ (a $\&$ b):
    use b 
\end{verbatim} \end{small}
The described approximation of the real edit cost may be too low, but never too
high. If a rule is allowed by our original set of rules, it may now be given a too
low cost or a erroneous (optimistic!) bitmap which in turn result in a lower
cost. However, all variants will be found, and always with an
initial (may grow higher) lower cost.
When the algorithm produces a anagram hash that is found in the lexicon, it
is passed on to the Levenshtein implementation. The top-$k$ results with a cost
lower than 2 are stored, and are returned whenever the filter returns a
suggestion with a higher cost.

\begin{figure}
\begin{small}
\begin{tabular}{ll}
\multicolumn{2}{l}{\textbf{n $\in$ V = <W,U,A,S,C,P>}}\\
W   &   weight \\
U   &   usedchars, list of bitmaps \\
A  &    anahash \\
S   &   siblings \\
C   &   children \\
P   &   parent \\
\end{tabular}
\caption{The graph representation} 
\end{small}
\end{figure}


\begin{small}
\begin{verbatim}
def dijkstrafind(originalrules,originalhash):

    rules = deltaizeetc(originalrules)
    pq    = priorityqueue

    pq.push(rootnode) # a root with weight and usedchars set to 0 
                      # all nodes as children but no siblings

    while has_element(pq):

        vertex = pop(pq)
        # return the first node in the priority queue
        yield (vertex.anahash,vertex.weight)

        # check for siblings, considering parents used characters
        sibling = nextpossiblerule(vertex.siblings, vertex.parent.usedchars
                                  ,rules)
        
            # create sibling and add
            pq.push(vertex.parent.weight+sibling.cost,
                    sibling.usedchars,
                    vertex.parent.anahash+sibling.delta_anahash,
                    sibling.children+1,
                    sibling.children,
                    vertex.parent)))
 
        # check for children, considering this nodes used characters
        child = nextpossiblerule(vertex.children, vertex.usedchars, rules)
                   
        # create child node and add
        pq.push(vertex.weight+child.cost,
                child.usedchars,
                vertex.anahash+child.children.delta_anahash,
                child.children+1,
                child.children,
                vertex)))
        
\end{verbatim}\end{small}



%Expressed as delta anahash (difference from last update), cost and a bitmask
%showing which letters that have been used.  delta anahash: how the word's
%letters are used. smallest possible set is used


%add child:
%add sibling:

%bitmaska ut bra varianter\\
%cannot reuse letters in word, but do not want to copy all nodes
%  possibly many ways the word can be edited (different set of letters used)
%  possibly many ways the rule can apply (the akutella letters may appear on
%    differnt positions in the word)
%filter the rules:
%not (lettersusedinRule \& lettersusedinWord)
%         two 1s will give 1
% both used anywhere -> 0
%$\lnot$ lr $\&$ lw  ... newusedinWord: lr $\|$ lw
%list of rules, travese it, in order so only keep tail

%optimistiskt, spara om under 2\\
%leads to optimistic filter, a is found at a lower or equal weigth.
%make clear!
%therefore, save 3 cheapest candidates under 2, if the filter suggest
%one with higher cost, return the first one

implemented in python/cython, using c for critical parts

\section{Stats, experiments}
Köra hela korpuset/1mln ord mot ordboken. \\
Information om hur många ord i genomsnitt kommer genom filtret
och hur många edits i genomsnitt det kräver \\
Köra korpuset mot sig själv.

\section*{Conclusion and perspectives (not numbered, use style “Heading1”)} %  not numbered
This section should better not be numbered (style “Heading1” inherits from “Heading 1”).

\section*{Acknowledgments (not numbered, use style Heading1)}
Optional section. If some named entities may lead to authors identification, please anonymize them.	\\

%'apalike-fr' style below applies smallcaps style on author names
%in order to apply 'apalike-fr' the babel package must be given [frenchb] option instead of [english]
% \usepackage[frenchb]{babel} also causes title "References" to render with French accents like "R\'ef\'erences"
%\bibliographystyle{apalike-fr}

%'apa' style does not apply "smallcaps style" on author names and goes with the [english] option in the babel package

\bibliographystyle{apa}

\bibliography{colingbiblio}
\nocite{TALN2007,LaigneletRioult09,LanglaisPatry07,au1972,cks1981,mb2012}

%%================================================================
\end{document}
